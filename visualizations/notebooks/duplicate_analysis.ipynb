{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Duplicate Analysis\n",
    "\n",
    "This notebook analyzes duplicate detection results and hash efficiency:\n",
    "- Duplicate detection overview\n",
    "- Inter-class vs intra-class duplicates\n",
    "- Hash efficiency metrics\n",
    "- Storage impact analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../visualizers')\n",
    "sys.path.append('../utils')\n",
    "\n",
    "from duplicate_detector import create_combined_duplicate_analysis\n",
    "from data_loader import load_report_data, get_duplicate_stats, get_overview_metrics\n",
    "from plot_helpers import apply_global_style, display_config\n",
    "\n",
    "apply_global_style()\n",
    "\n",
    "data = load_report_data('../../sample_report.json')\n",
    "print(\"Data loaded successfully!\")\n",
    "print(f\"Generated at: {data.get('generated_at', 'Unknown')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duplicate Detection Summary\n",
    "\n",
    "High-level duplicate detection metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "charts = create_combined_duplicate_analysis(data)\n",
    "\n",
    "summary_fig = charts['summary_cards']\n",
    "summary_fig.show(config=display_config())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duplicate Detection Overview\n",
    "\n",
    "Unique vs duplicate image distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overview_fig = charts['duplicate_overview']\n",
    "overview_fig.show(config=display_config())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hash Efficiency Metrics\n",
    "\n",
    "Effectiveness of the duplicate detection system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "efficiency_fig = charts['efficiency_metrics']\n",
    "efficiency_fig.show(config=display_config())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duplicate Type Breakdown\n",
    "\n",
    "Detailed analysis of different duplicate types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breakdown_fig = charts['duplicate_breakdown']\n",
    "breakdown_fig.show(config=display_config())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inter-Class vs Intra-Class Comparison\n",
    "\n",
    "Distribution of duplicates within and across classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_fig = charts['class_comparison']\n",
    "comparison_fig.show(config=display_config())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duplicate Detection Funnel\n",
    "\n",
    "Flow from total images to duplicate identification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "funnel_fig = charts['distribution_funnel']\n",
    "funnel_fig.show(config=display_config())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impact Analysis\n",
    "\n",
    "Storage and processing impact of duplicates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impact_fig = charts['impact_analysis']\n",
    "impact_fig.show(config=display_config())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duplicate Statistics Report\n",
    "\n",
    "Comprehensive duplicate analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_stats = get_duplicate_stats(data)\n",
    "overview_metrics = get_overview_metrics(data)\n",
    "duplicate_summary = duplicate_stats['duplicate_summary']\n",
    "\n",
    "print(\"=== DUPLICATE DETECTION RESULTS ===\")\n",
    "print(f\"Total Images Processed: {duplicate_stats['total_count']:,}\")\n",
    "print(f\"Unique Images: {duplicate_stats['unique_count']:,}\")\n",
    "print(f\"Duplicate Hashes: {duplicate_stats['duplicate_count']:,}\")\n",
    "\n",
    "uniqueness_rate = (duplicate_stats['unique_count'] / duplicate_stats['total_count']) * 100\n",
    "duplicate_rate = (duplicate_stats['duplicate_count'] / duplicate_stats['total_count']) * 100\n",
    "\n",
    "print(f\"\\nUniqueness Rate: {uniqueness_rate:.2f}%\")\n",
    "print(f\"Duplicate Rate: {duplicate_rate:.2f}%\")\n",
    "\n",
    "print(\"\\n=== DUPLICATE TYPE BREAKDOWN ===\")\n",
    "print(f\"Inter-class Duplicates: {duplicate_summary.get('inter_class_duplicate_hashes', 0):,}\")\n",
    "print(f\"Intra-class Duplicates: {duplicate_summary.get('intra_class_duplicate_hashes', 0):,}\")\n",
    "print(f\"Total Duplicate Hashes: {duplicate_summary.get('total_duplicate_hashes', 0):,}\")\n",
    "print(f\"Total Duplicate Files: {duplicate_summary.get('total_duplicate_files', 0):,}\")\n",
    "\n",
    "# Calculate storage impact\n",
    "total_duplicate_files = duplicate_summary.get('total_duplicate_files', 0)\n",
    "if total_duplicate_files > 0 and overview_metrics['total_images'] > 0:\n",
    "    storage_waste = (total_duplicate_files / overview_metrics['total_images']) * 100\n",
    "    avg_file_size_mb = overview_metrics['avg_file_size_mb']\n",
    "    wasted_storage_mb = total_duplicate_files * avg_file_size_mb\n",
    "    \n",
    "    print(f\"\\n=== STORAGE IMPACT ===\")\n",
    "    print(f\"Estimated Storage Waste: {storage_waste:.2f}%\")\n",
    "    print(f\"Approximate Wasted Storage: {wasted_storage_mb:.1f} MB\")\n",
    "    \n",
    "    if wasted_storage_mb > 1024:\n",
    "        print(f\"                          {wasted_storage_mb/1024:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality Assessment\n",
    "\n",
    "Evaluation of duplicate detection effectiveness:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== DUPLICATE DETECTION QUALITY ASSESSMENT ===\")\n",
    "\n",
    "if uniqueness_rate >= 99:\n",
    "    print(\"üü¢ Excellent uniqueness - minimal duplicates detected\")\n",
    "    print(\"   Dataset quality: Outstanding\")\n",
    "elif uniqueness_rate >= 95:\n",
    "    print(\"üü° Good uniqueness - acceptable duplicate levels\")\n",
    "    print(\"   Dataset quality: Good\")\n",
    "elif uniqueness_rate >= 90:\n",
    "    print(\"üü† Moderate uniqueness - noticeable duplicates\")\n",
    "    print(\"   Dataset quality: Fair - consider cleanup\")\n",
    "else:\n",
    "    print(\"üî¥ Low uniqueness - significant duplicate problem\")\n",
    "    print(\"   Dataset quality: Poor - cleanup recommended\")\n",
    "\n",
    "# Analyze duplicate patterns\n",
    "inter_class = duplicate_summary.get('inter_class_duplicate_hashes', 0)\n",
    "intra_class = duplicate_summary.get('intra_class_duplicate_hashes', 0)\n",
    "total_dup_hashes = duplicate_summary.get('total_duplicate_hashes', 0)\n",
    "\n",
    "if total_dup_hashes > 0:\n",
    "    inter_ratio = (inter_class / total_dup_hashes) * 100\n",
    "    intra_ratio = (intra_class / total_dup_hashes) * 100\n",
    "    \n",
    "    print(f\"\\n=== DUPLICATE PATTERN ANALYSIS ===\")\n",
    "    print(f\"Inter-class Duplicates: {inter_ratio:.1f}% of total duplicates\")\n",
    "    print(f\"Intra-class Duplicates: {intra_ratio:.1f}% of total duplicates\")\n",
    "    \n",
    "    if inter_class > intra_class:\n",
    "        print(\"\\n‚ö†Ô∏è  High inter-class duplication detected:\")\n",
    "        print(\"   - Same images appearing in multiple classes\")\n",
    "        print(\"   - May impact model training accuracy\")\n",
    "        print(\"   - Consider reviewing class definitions\")\n",
    "    else:\n",
    "        print(\"\\n‚úÖ Duplicates mostly within classes:\")\n",
    "        print(\"   - Normal pattern for class-based collection\")\n",
    "        print(\"   - Lower impact on model training\")\n",
    "else:\n",
    "    print(\"\\nüéâ No duplicates detected - perfect uniqueness!\")\n",
    "\n",
    "print(\"\\n=== RECOMMENDATIONS ===\")\n",
    "\n",
    "if duplicate_rate < 1:\n",
    "    print(\"‚úÖ Excellent duplicate detection - no action needed\")\n",
    "elif duplicate_rate < 5:\n",
    "    print(\"üìä Consider implementing duplicate removal for:\")\n",
    "    print(\"   - Storage optimization\")\n",
    "    print(\"   - Training efficiency improvement\")\n",
    "else:\n",
    "    print(\"üîß Recommend implementing duplicate cleanup:\")\n",
    "    print(\"   - Automated deduplication pipeline\")\n",
    "    print(\"   - Enhanced hash-based filtering\")\n",
    "    print(\"   - Manual review of flagged duplicates\")\n",
    "\n",
    "if inter_class > 0:\n",
    "    print(\"\\nüéØ Inter-class Duplicate Action Items:\")\n",
    "    print(\"   - Review and validate class boundaries\")\n",
    "    print(\"   - Implement cross-class deduplication\")\n",
    "    print(\"   - Consider data source overlap analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Options\n",
    "\n",
    "Save duplicate analysis reports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to save charts\n",
    "# from plot_helpers import save_plot\n",
    "# \n",
    "# save_plot(summary_fig, 'duplicate_summary', 'png', width=1200, height=400)\n",
    "# save_plot(overview_fig, 'duplicate_overview', 'png', width=800, height=600)\n",
    "# save_plot(efficiency_fig, 'hash_efficiency', 'png', width=1200, height=500)\n",
    "# save_plot(breakdown_fig, 'duplicate_breakdown', 'png', width=1000, height=600)\n",
    "# save_plot(funnel_fig, 'duplicate_funnel', 'png', width=800, height=600)\n",
    "# save_plot(impact_fig, 'duplicate_impact', 'html')\n",
    "# \n",
    "# print(\"Duplicate analysis charts saved to visualizations/output/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
